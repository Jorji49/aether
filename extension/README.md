# Aether â€” AI Prompt Optimizer

> Transform rough ideas into perfect AI prompts. Powered by local Ollama models â€” your data stays private.

![Visual Studio Marketplace](https://img.shields.io/visual-studio-marketplace/v/kama.aether)
![License](https://img.shields.io/github/license/Jorji49/aether)

## âœ¨ Features

- **Vibe-to-Prompt**: Type a rough idea like "login sayfasÄ± yap" and get a fully structured, production-ready AI prompt
- **100% Local & Private**: Runs entirely on your machine via Ollama â€” no data leaves your computer
- **Multi-IDE Support**: Works with Cursor, Windsurf, Claude Code, GitHub Copilot, and VS Code
- **Smart Context Scanning**: Automatically analyzes your project structure for context-aware prompts
- **Agent Selector**: Choose your target AI model (Claude, GPT, Gemini, Grok) for optimized prompt formatting
- **Quality Scoring**: Every generated prompt gets a quality grade (A+, A, B, C, D)
- **Security Auditing**: Prompts are checked for injection risks and sensitive data leaks
- **One-Click Model Management**: Download, switch, and manage Ollama models from the sidebar
- **Send to Agent**: Instantly send generated prompts to your IDE's AI assistant

## ğŸš€ Quick Start

### Prerequisites

1. **[Ollama](https://ollama.com)** â€” Install and run locally
2. **Python 3.10+** â€” For the Brain backend server

### Setup

1. Install the Aether extension from the VS Code Marketplace
2. Open a project folder in your IDE
3. Run the command **"Aether: Start Brain"** from the Command Palette (`Ctrl+Shift+P`)
4. The extension will:
   - Start Ollama (if not running)
   - Install Python dependencies
   - Launch the Brain server on port 8420
5. Select a model in the Welcome screen (recommended: **gemma3:4b**)
6. Start typing your ideas!

## ğŸ¯ How It Works

```
Your Idea â†’ Aether Brain â†’ Ollama (Local LLM) â†’ Optimized Prompt â†’ Your AI Agent
```

1. **You type** a rough idea in the Aether sidebar
2. **Context Scanner** analyzes your project (files, structure, tech stack)
3. **Prompt Optimizer** crafts a detailed, structured prompt using the local LLM
4. **Quality Auditor** scores the output and checks for security issues
5. **One click** sends the prompt to your AI agent (Cursor, Copilot, etc.)

## âš™ï¸ Configuration

| Setting | Default | Description |
|---------|---------|-------------|
| `aether.brainServerUrl` | `http://127.0.0.1:8420` | Brain server URL |
| `aether.ollamaModel` | `gemma2:2b` | Ollama model for prompt generation |
| `aether.maxContextFiles` | `30` | Max project files to scan for context |
| `aether.autoSendToAgent` | `false` | Auto-send prompt to AI agent |
| `aether.ollamaTemperature` | `0.1` | Generation temperature (0.0â€“1.0) |
| `aether.ollamaMaxTokens` | `1024` | Max tokens for generated prompts |

## ğŸ“¦ Recommended Models

| Model | Size | Speed | Quality | Best For |
|-------|------|-------|---------|----------|
| gemma3:4b | 3.3 GB | âš¡âš¡ | â­â­â­ | Best balance (recommended) |
| gemma2:2b | 1.6 GB | âš¡âš¡âš¡ | â­â­ | Fast iterations |
| gemma3:1b | 815 MB | âš¡âš¡âš¡âš¡ | â­ | Ultra-fast, low RAM |
| phi4 | 9.1 GB | âš¡ | â­â­â­â­ | Highest quality |
| llama3.1:8b | 4.7 GB | âš¡ | â­â­â­â­ | Strong general use |

## ğŸ”’ Privacy

Aether is designed with privacy as a core principle:

- **Zero cloud dependency** â€” All processing happens locally
- **No telemetry** â€” We don't collect any usage data
- **No API keys needed** â€” Works entirely with local Ollama models
- **Your code stays yours** â€” Context scanning is local-only

## ğŸ—ï¸ Architecture

```
aether/
â”œâ”€â”€ brain/              # Python backend (FastAPI)
â”‚   â”œâ”€â”€ sslm_engine.py          # Main server & Ollama integration
â”‚   â”œâ”€â”€ prompt_optimizer.py     # Prompt generation engine
â”‚   â”œâ”€â”€ prompt_knowledge_base.py # Best practices & patterns
â”‚   â”œâ”€â”€ context_scanner.py      # Project analysis
â”‚   â”œâ”€â”€ security_auditor.py     # Security checks
â”‚   â””â”€â”€ config.py               # Configuration
â””â”€â”€ extension/          # VS Code extension (TypeScript)
    â””â”€â”€ src/
        â”œâ”€â”€ extension.ts            # Entry point
        â”œâ”€â”€ providers/SidebarProvider.ts  # UI
        â”œâ”€â”€ services/BrainClient.ts      # HTTP client
        â””â”€â”€ utils/config.ts              # Settings
```

## ğŸ“ Commands

| Command | Description |
|---------|-------------|
| `Aether: Send Vibe` | Enter a prompt idea via input box |
| `Aether: Start Brain` | Start the Ollama + Brain server |
| `Aether: Send to Agent` | Send a prompt to your AI assistant |

## ğŸ› Troubleshooting

### Brain server won't start
- Make sure Python 3.10+ is installed: `python --version`
- Make sure Ollama is installed: `ollama --version`
- Check if port 8420 is available

### No models showing up
- Ensure Ollama is running: `ollama serve`
- Try pulling a model manually: `ollama pull gemma2:2b`

### Extension not connecting
- Check the Brain server URL in settings
- The extension auto-discovers ports 8420â€“8429

## ğŸ“„ License

MIT Â© [kama](https://github.com/Jorji49)
