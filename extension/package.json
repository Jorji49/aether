{
  "name": "aether",
  "displayName": "Aether - AI Prompt Optimizer",
  "description": "Transform rough ideas into perfect AI prompts. Powered by local Ollama models - your data stays private.",
  "version": "1.0.0",
  "publisher": "AhmetKayraKama",
  "icon": "media/aether-icon.png",
  "repository": {
    "type": "git",
    "url": "https://github.com/Jorji49/aether"
  },
  "homepage": "https://github.com/Jorji49/aether#readme",
  "bugs": {
    "url": "https://github.com/Jorji49/aether/issues"
  },
  "license": "MIT",
  "engines": {
    "vscode": "^1.96.0"
  },
  "galleryBanner": {
    "color": "#000000",
    "theme": "dark"
  },
  "categories": [
    "Programming Languages",
    "Machine Learning",
    "Other"
  ],
  "keywords": [
    "ai",
    "prompt",
    "ollama",
    "claude",
    "gpt",
    "gemini",
    "prompt-engineering",
    "local-ai",
    "privacy"
  ],
  "activationEvents": [],
  "main": "./out/extension.js",
  "contributes": {
    "viewsContainers": {
      "activitybar": [
        {
          "id": "aether-sidebar",
          "title": "Aether",
          "icon": "media/aether-icon.svg"
        }
      ]
    },
    "views": {
      "aether-sidebar": [
        {
          "type": "webview",
          "id": "aether.vibePanel",
          "name": "Aether"
        }
      ]
    },
    "commands": [
      {
        "command": "aether.sendVibe",
        "title": "Aether: Send Vibe"
      },
      {
        "command": "aether.startBrain",
        "title": "Aether: Start Brain"
      },
      {
        "command": "aether.sendToAgent",
        "title": "Aether: Send to Agent"
      }
    ],
    "configuration": {
      "title": "Aether",
      "properties": {
        "aether.brainServerUrl": {
          "type": "string",
          "default": "http://127.0.0.1:8420",
          "description": "URL of the Aether Brain server. Change only if running Brain on a different machine or port.",
          "order": 1
        },
        "aether.ollamaModel": {
          "type": "string",
          "default": "gemma2:2b",
          "description": "Ollama model used for prompt generation. Smaller models (gemma2:2b, gemma3:1b) are faster. Larger models (phi4, llama3.1:8b) produce higher quality prompts.",
          "order": 2
        },
        "aether.maxContextFiles": {
          "type": "number",
          "default": 30,
          "description": "Maximum number of project files to scan for context. Higher values give more context but slower generation. Recommended: 20-50.",
          "minimum": 5,
          "maximum": 100,
          "order": 3
        },
        "aether.autoSendToAgent": {
          "type": "boolean",
          "default": false,
          "description": "Automatically send the generated prompt to the AI agent after generation.",
          "order": 4
        },
        "aether.ollamaTemperature": {
          "type": "number",
          "default": 0.1,
          "description": "Temperature for prompt generation (0.0-1.0). Lower = more focused. Recommended: 0.1 for coding prompts.",
          "minimum": 0,
          "maximum": 1,
          "order": 5
        },
        "aether.ollamaMaxTokens": {
          "type": "number",
          "default": 1024,
          "description": "Maximum tokens for generated prompts. Recommended: 768-1024 for most tasks, 1536+ for complex prompts.",
          "minimum": 256,
          "maximum": 4096,
          "order": 6
        }
      }
    }
  },
  "scripts": {
    "vscode:prepublish": "npm run compile",
    "compile": "tsc -p ./",
    "watch": "tsc -watch -p ./",
    "lint": "eslint src --ext ts",
    "test": "node ./out/test/runTest.js"
  },
  "devDependencies": {
    "@types/node": "^22.0.0",
    "@types/vscode": "^1.96.0",
    "@typescript-eslint/eslint-plugin": "^8.0.0",
    "@typescript-eslint/parser": "^8.0.0",
    "eslint": "^8.57.0",
    "typescript": "^5.7.0"
  }
}
